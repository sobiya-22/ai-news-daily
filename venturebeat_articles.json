[
    {
        "title": "5 key questions your developers should be asking about MCP",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nThe Model Context Protocol (MCP) has become one of the most talked-about developments in AI integration since its introduction by Anthropic in late 2024. If you’re tuned into the AI space at all, you’ve likely been inundated with developer “hot takes” on the topic. Some think it’s the best thing ever; others are quick to point out its shortcomings. In reality, there’s some truth to both.\nOne pattern I’ve noticed with MCP adoption is that skepticism typically gives way to recognition: This protocol solves genuine architectural problems that other approaches don’t. I’ve gathered a list of questions below that reflect the conversations I’ve had with fellow builders who are considering bringing MCP to production environments. \n1. Why should I use MCP over other alternatives?\nOf course, most developers considering MCP are already familiar with implementations like OpenAI’s custom GPTs, vanilla function calling, Responses API with function calling, and hardcoded connections to services like Google Drive. The question isn’t really whether MCP fully replaces these approaches — under the hood, you could absolutely use the Responses API with function calling that still connects to MCP. What matters here is the resulting stack.\nDespite all the hype about MCP, here’s the straight truth: It’s not a massive technical leap. MCP essentially “wraps” existing APIs in a way that’s understandable to large language models (LLMs). Sure, a lot of services already have an OpenAPI spec that models can use. For small or personal projects, the objection that MCP “isn’t that big a deal” is pretty fair.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited: https://bit.ly/3GuuPLF\nThe practical benefit becomes obvious when you’re building something like an analysis tool that needs to connect to data sources across multiple ecosystems. Without MCP, you’re required to write custom integrations for each data source and each LLM you want to support. With MCP, you implement the data source connections once, and any compatible AI client can use them.\n2. Local vs. remote MCP deployment: What are the actual trade-offs in production?\nThis is where you really start to see the gap between reference servers and reality. Local MCP deployment using the stdio programming language is dead simple to get running: Spawn subprocesses for each MCP server and let them talk through stdin/stdout. Great for a technical audience, difficult for everyday users.\nRemote deployment obviously addresses the scaling but opens up a can of worms around transport complexity. The original HTTP+SSE approach was replaced by a March 2025 streamable HTTP update, which tries to reduce complexity by putting everything through a single /messages endpoint. Even so, this isn’t really needed for most companies that are likely to build MCP servers.\nBut here’s the thing: A few months later, support is spotty at best. Some clients still expect the old HTTP+SSE setup, while others work with the new approach — so, if you’re deploying today, you’re probably going to support both. Protocol detection and dual transport support are a must.\nAuthorization is another variable you’ll need to consider with remote deployments. The OAuth 2.1 integration requires mapping tokens between external identity providers and MCP sessions. While this adds complexity, it’s manageable with proper planning.\n3. How can I be sure my MCP server is secure?\nThis is probably the biggest gap between the MCP hype and what you actually need to tackle for production. Most showcases or examples you’ll see use local connections with no authentication at all, or they handwave the security by saying “it uses OAuth.” \nThe MCP authorization spec does leverage OAuth 2.1, which is a proven open standard. But there’s always going to be some variability in implementation. For production deployments, focus on the fundamentals: \nProper scope-based access control that matches your actual tool boundaries \nDirect (local) token validation\nAudit logs and monitoring for tool use\nHowever, the biggest security consideration with MCP is around tool execution itself. Many tools need (or think they need) broad permissions to be useful, which means sweeping scope design (like a blanket “read” or “write”) is inevitable. Even without a heavy-handed approach, your MCP server may access sensitive data or perform privileged operations — so, when in doubt, stick to the best practices recommended in the latest MCP auth draft spec.\n4. Is MCP worth investing resources and time into, and will it be around for the long term?\nThis gets to the heart of any adoption decision: Why should I bother with a flavor-of-the-quarter protocol when everything AI is moving so fast? What guarantee do you have that MCP will be a solid choice (or even around) in a year, or even six months? \nWell, look at MCP’s adoption by major players: Google supports it with its Agent2Agent protocol, Microsoft has integrated MCP with Copilot Studio and is even adding built-in MCP features for Windows 11, and Cloudflare is more than happy to help you fire up your first MCP server on their platform. Similarly, the ecosystem growth is encouraging, with hundreds of community-built MCP servers and official integrations from well-known platforms. \nIn short, the learning curve isn’t terrible, and the implementation burden is manageable for most teams or solo devs. It does what it says on the tin. So, why would I be cautious about buying into the hype?\nMCP is fundamentally designed for current-gen AI systems, meaning it assumes you have a human supervising a single-agent interaction. Multi-agent and autonomous tasking are two areas MCP doesn’t really address; in fairness, it doesn’t really need to. But if you’re looking for an evergreen yet still somehow bleeding-edge approach, MCP isn’t it. It’s standardizing something that desperately needs consistency, not pioneering in uncharted territory.\n5. Are we about to witness the “AI protocol wars?”\nSigns are pointing toward some tension down the line for AI protocols. While MCP has carved out a tidy audience by being early, there’s plenty of evidence it won’t be alone for much longer.\nTake Google’s Agent2Agent (A2A) protocol launch with 50-plus industry partners. It’s complementary to MCP, but the timing — just weeks after OpenAI publicly adopted MCP — doesn’t feel coincidental. Was Google cooking up an MCP competitor when they saw the biggest name in LLMs embrace it? Maybe a pivot was the right move. But it’s hardly speculation to think that, with features like multi-LLM sampling soon to be released for MCP, A2A and MCP may become competitors.\nThen there’s the sentiment from today’s skeptics about MCP being a “wrapper” rather than a genuine leap forward for API-to-LLM communication. This is another variable that will only become more apparent as consumer-facing applications move from single-agent/single-user interactions and into the realm of multi-tool, multi-user, multi-agent tasking. What MCP and A2A don’t address will become a battleground for another breed of protocol altogether.\nFor teams bringing AI-powered projects to production today, the smart play is probably hedging protocols. Implement what works now while designing for flexibility. If AI makes a generational leap and leaves MCP behind, your work won’t suffer for it. The investment in standardized tool integration absolutely will pay off immediately, but keep your architecture adaptable for whatever comes next.\nUltimately, the dev community will decide whether MCP stays relevant. It’s MCP projects in production, not specification elegance or market buzz, that will determine if MCP (or something else) stays on top for the next AI hype cycle. And frankly, that’s probably how it should be.\nMeir Wahnon is a co-founder at Descope.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/ai/5-key-questions-your-developers-should-be-asking-about-mcp/",
        "scraped_at": "2025-07-21T13:17:50.846765",
        "article_type": "featured"
    },
    {
        "title": "How Capital One built production multi-agent AI workflows to power enterprise use cases",
        "content": "How do you balance risk management and safety with innovation in agentic systems — and how do you grapple with core considerations around data and model selection? In this VB Transform session, Milind Naphade, SVP, technology, of AI Foundations at Capital One, offered best practices and lessons learned from real-world experiments and applications for deploying and scaling an agentic workflow.\nCapital One, committed to staying at the forefront of emerging technologies, recently launched a production-grade, state-of-the-art multi-agent AI system to enhance the car-buying experience. In this system, multiple AI agents work together to not only provide information to the car buyer, but to take specific actions based on the customer’s preferences and needs. For example, one agent communicates with the customer. Another creates an action plan based on business rules and the tools it is allowed to use. A third agent evaluates the accuracy of the first two, and a fourth agent explains and validates the action plan with the user. With over 100 million customers using a wide range of other potential Capital One use case applications, the agentic system is built for scale and complexity.\n“When we think of improving the customer experience, delighting the customer, we think of, what are the ways in which that can happen?” Naphade said. “Whether you’re opening an account or you want to know your balance or you’re trying to make a reservation to test a vehicle, there are a bunch of things that customers want to do. At the heart of this, very simply, how do you understand what the customer wants? How do you understand the fulfillment mechanisms at your disposal? How do you bring all the rigors of a regulated entity like Capital One, all the policies, all the business rules, all the constraints, regulatory and otherwise?”\nAgentic AI was clearly the next step, he said, for internal as well as customer-facing use cases.\nDesigning an agentic workflow\nFinancial institutions have particularly stringent requirements when designing any workflow that supports customer journeys. And Capital One’s applications include a number of complex processes as customers raise issues and queries leveraging conversational tools. These two factors made the design process especially complex, requiring a holistic view of the entire journey — including how both customers and human agents respond, react, and reason at every step.\n“When we looked at how humans do reasoning, we were struck by a few salient facts,” Naphade said. “We saw that if we designed it using multiple logical agents, we would be able to mimic human reasoning quite well. But then you ask yourself, what exactly do the different agents do? Why do you have four? Why not three? Why not 20?”\nThey studied customer experiences in the historic data: where those conversations go right, where they go wrong, how long they should take and other salient facts. They learned that it often takes multiple turns of conversation with an agent to understand what the customer wants, and any agentic workflow needs to plan for that, but also be completely grounded in an organization’s systems, available tools, APIs, and organizational policy guardrails.\n“The main breakthrough for us was realizing that this had to be dynamic and iterative,” Naphade said. “If you look at how a lot of people are using LLMs, they’re slapping the LLMs as a front end to the same mechanism that used to exist. They’re just using LLMs for classification of intent. But we realized from the beginning that that was not scalable.”\nTaking cues from existing workflows\nBased on their intuition of how human agents reason while responding to customers, researchers at Capital One developed  a framework in which  a team of expert AI agents, each with different expertise, come together and solve a problem.\nAdditionally, Capital One incorporated robust risk frameworks into the development of the agentic system. As a regulated institution, Naphade noted that in addition to its range of internal risk mitigation protocols and frameworks,”Within Capital One, to manage risk, other entities that are independent observe you, evaluate you, question you, audit you,” Naphade said. “We thought that was a good idea for us, to have an AI agent whose entire job was to evaluate what the first two agents do based on Capital One policies and rules.”\nThe evaluator determines whether the earlier agents were successful, and if not, rejects the plan and requests the planning agent to correct its results based on its judgement of where the problem was. This happens in an iterative process until the appropriate plan is reached. It’s also proven to be a huge boon to the company’s agentic AI approach.\n“The evaluator agent is … where we bring a world model. That’s where we simulate what happens if a series of actions were to be actually executed. That kind of rigor, which we need because we are a regulated enterprise – I think that’s actually putting us on a great sustainable and robust trajectory. I expect a lot of enterprises will eventually go to that point.”\nThe technical challenges of agentic AI\nAgentic systems need to work with fulfillment systems across the organization, all with a variety of permissions. Invoking tools and APIs within a variety of contexts while maintaining high accuracy was also challenging — from disambiguating user intent to generating and executing a reliable plan.\n“We have multiple iterations of experimentation, testing, evaluation, human-in-the-loop, all the right guardrails that need to happen before we can actually come into the market with something like this,” Naphade said. “But one of the biggest challenges was we didn’t have any precedent. We couldn’t go and say, oh, somebody else did it this way. How did that work out? There was that element of novelty. We were doing it for the first time.”\nModel selection and partnering with NVIDIA\nIn terms of models, Capital One is keenly tracking academic and industry research, presenting at conferences and staying abreast of what’s state of the art. In the present use case, they used open-weights models, rather than closed, because that allowed them significant customization. That’s critical to them, Naphade asserts, because competitive advantage in AI strategy relies on proprietary data.\nIn the technology stack itself, they use a combination of tools, including in-house technology, open-source tool chains, and NVIDIA inference stack. Working closely with NVIDIA has helped Capital One get the performance they need, and collaborate on industry-specific  opportunities in NVIDIA’s library, and prioritize features for the Triton server and their TensoRT LLM.\nAgentic AI: Looking ahead\nCapital One continues to deploy, scale, and refine AI agents across their business. Their first multi-agentic workflow was Chat Concierge, deployed through the company’s auto business. It was designed to support both auto dealers and customers with the car-buying process.  And with rich customer data, dealers are identifying serious leads, which has improved their customer engagement metrics significantly — up to 55% in some cases.\n“They’re able to generate much better serious leads through this natural, easier, 24/7 agent working for them,” Naphade said. “We’d like to bring this capability to [more] of our customer-facing engagements. But we want to do it in a well-managed way. It’s a journey.”",
        "source_url": "https://venturebeat.com/ai/how-capital-one-built-production-multi-agent-ai-workflows-to-power-enterprise-use-cases/",
        "scraped_at": "2025-07-21T13:17:52.956788",
        "article_type": "featured"
    },
    {
        "title": "New embedding model leaderboard shakeup: Google takes #1 while Alibaba’s open source alternative closes gap",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nGoogle has officially moved its new, high-performance Gemini Embedding model to general availability, currently ranking number one overall on the highly regarded Massive Text Embedding Benchmark (MTEB). The model (gemini-embedding-001) is now a core part of the Gemini API and Vertex AI, enabling developers to build applications such as semantic search and retrieval-augmented generation (RAG).\nWhile a number-one ranking is a strong debut, the landscape of embedding models is very competitive. Google’s proprietary model is being challenged directly by powerful open-source alternatives. This sets up a new strategic choice for enterprises: adopt the top-ranked proprietary model or a nearly-as-good open-source challenger that offers more control.\nFBI Veterans on AI Cyber Threats & Future Defenders\nWhat’s under the hood of Google’s Gemini embedding model\nAt their core, embeddings convert text (or other data types) into numerical lists that capture the key features of the input. Data with similar semantic meaning have embedding values that are closer together in this numerical space. This allows for powerful applications that go far beyond simple keyword matching, such as building intelligent retrieval-augmented generation (RAG) systems that feed relevant information to LLMs. \nEmbeddings can also be applied to other modalities such as images, video and audio. For instance, an e-commerce company might utilize a multimodal embedding model to generate a unified numerical representation for a product that incorporates both textual descriptions and images.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited: https://bit.ly/3GuuPLF\nFor enterprises, embedding models can power more accurate internal search engines, sophisticated document clustering, classification tasks, sentiment analysis and anomaly detection. Embeddings are also becoming an important part of agentic applications, where AI agents must retrieve and match different types of documents and prompts.\nOne of the key features of Gemini Embedding is its built-in flexibility. It has been trained through a technique known as Matryoshka Representation Learning (MRL), which allows developers to get a highly detailed 3072-dimension embedding but also truncate it to smaller sizes like 1536 or 768 while preserving its most relevant features. This flexibility enables an enterprise to strike a balance between model accuracy, performance and storage costs, which is crucial for scaling applications efficiently.\nGoogle positions Gemini Embedding as a unified model designed to work effectively “out-of-the-box” across diverse domains like finance, legal and engineering without the need for fine-tuning. This simplifies development for teams that need a general-purpose solution. Supporting over 100 languages and priced competitively at $0.15 per million input tokens, it is designed for broad accessibility.\nA competitive landscape of proprietary and open-source challengers\nSource: Google Blog\nADVERTISEMENT\nThe MTEB leaderboard shows that while Gemini leads, the gap is narrow. It faces established models from OpenAI, whose embedding models are widely used, and specialized challengers like Mistral, which offers a model specifically for code retrieval. The emergence of these specialized models suggests that for certain tasks, a targeted tool may outperform a generalist one.\nAnother key player, Cohere, targets the enterprise directly with its Embed 4 model. While other models compete on general benchmarks, Cohere emphasizes its model’s ability to handle the “noisy real-world data” often found in enterprise documents, such as spelling mistakes, formatting issues, and even scanned handwriting. It also offers deployment on virtual private clouds or on-premises, providing a level of data security that directly appeals to regulated industries such as finance and healthcare.\nThe most direct threat to proprietary dominance comes from the open-source community. Alibaba’s Qwen3-Embedding model ranks just behind Gemini on MTEB and is available under a permissive Apache 2.0 license (available for commercial purposes). For enterprises focused on software development, Qodo’s Qodo-Embed-1-1.5B presents another compelling open-source alternative, designed specifically for code and claiming to outperform larger models on domain-specific benchmarks.\nFor companies already building on Google Cloud and the Gemini family of models, adopting the native embedding model can have several benefits, including seamless integration, a simplified MLOps pipeline, and the assurance of using a top-ranked general-purpose model.\nHowever, Gemini is a closed, API-only model. Enterprises that prioritize data sovereignty, cost control, or the ability to run models on their own infrastructure now have a credible, top-tier open-source option in Qwen3-Embedding or can use one of the task-specific embedding models.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/ai/new-embedding-model-leaderboard-shakeup-google-takes-1-while-alibabas-open-source-alternative-closes-gap/",
        "scraped_at": "2025-07-21T13:17:55.052500",
        "article_type": "regular"
    },
    {
        "title": "How OpenAI’s red team made ChatGPT agent into an AI fortress",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nIn case you missed it, OpenAI yesterday debuted a powerful new feature for ChatGPT and with it, a host of new security risks and ramifications.\nCalled the “ChatGPT agent,” this new feature is an optional mode that ChatGPT paying subscribers can engage by clicking “Tools” in the prompt entry box and selecting “agent mode,” at which point, they can ask ChatGPT to log into their email and other web accounts; write and respond to emails; download, modify, and create files; and do a host of other tasks on their behalf, autonomously, much like a real person using a computer with their login credentials.\nFBI Veterans on AI Cyber Threats & Future Defenders\nObviously, this also requires the user to trust the ChatGPT agent not to do anything problematic or nefarious, or to leak their data and sensitive information. It also poses greater risks for a user and their employer than the regular ChatGPT, which can’t log into web accounts or modify files directly.\nKeren Gu, a member of the Safety Research team at OpenAI, commented on X that “we’ve activated our strongest safeguards for ChatGPT Agent. It’s the first model we’ve classified as High capability in biology & chemistry under our Preparedness Framework. Here’s why that matters–and what we’re doing to keep it safe.”\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited: https://bit.ly/3GuuPLF\nSo how did OpenAI handle all these security issues?\nThe red team’s mission\nLooking at OpenAI’s ChatGPT agent system card, the “read team” employed by the company to test the feature faced a challenging mission: specifically, 16 PhD security researchers who were given 40 hours to test it out.\nThrough systematic testing, the red team discovered seven universal exploits that could compromise the system, revealing critical vulnerabilities in how AI agents handle real-world interactions.\nWhat followed next was extensive security testing, much of it predicated on red teaming. The Red Teaming Network submitted 110 attacks, from prompt injections to biological information extraction attempts. Sixteen exceeded internal risk thresholds. Each finding gave OpenAI engineers the insights they needed to get fixes written and deployed before launch.\nThe results speak for themselves in the published results in the system card. ChatGPT Agent emerged with significant security improvements, including 95% performance against visual browser irrelevant instruction attacks and robust biological and chemical safeguards.\nRed teams exposed seven universal exploits\nOpenAI’s Red Teaming Network was comprised 16 researchers with biosafety-relevant PhDs who topgether submitted 110 attack attempts during the testing period. Sixteen exceeded internal risk thresholds, revealing fundamental vulnerabilities in how AI agents handle real-world interactions. But the real breakthrough came from UK AISI’s unprecedented access to ChatGPT Agent’s internal reasoning chains and policy text. Admittedly that’s intelligence regular attackers would never possess.\nOver four testing rounds, UK AISI forced OpenAI to execute seven universal exploits that had the potential to compromise any conversation:\nAttack vectors that forced OpenAI’s hand\nAttack Type Success Rate (Pre-Fix) Target Impact\nVisual Browser Hidden Instructions 33% Web pages Active data exfiltration\nGoogle Drive Connector Exploitation Not disclosed Cloud documents Forced document leaks\nMulti-Step Chain Attacks Variable Cross-site actions Complete session compromise\nBiological Information Extraction 16 submissions exceeded thresholds Dangerous knowledge Potential weaponization\nFAR.AI’s assessment was openly critical of OpenAI’s approach. Despite 40 hours of testing revealing only three partial vulnerabilities, they identified that current safety mechanisms relied heavily on monitoring during reasoning and tool-use processes, which the researchers considered a potential single point of failure if compromised.\nHow red teaming helped turn ChatGPT vulnerabilities into a fortress\nOpenAI’s response to the red team’s results redefined entire segments of the ChatGPT agent architecture. One of the many initiatives undertaken including building a dual-layer inspection architecture that monitors 100% of production traffic in real-time, achieving these measurable improvements:\nSecurity improvements after red team discoveries\nDefense Metric Previous Models ChatGPT Agent Improvement\nIrrelevant Instructions (Visual Browser) 82% 95% +13%\nIn-Context Data Exfiltration 75% 78% +3%\nActive Data Exfiltration 58% 67% +9%\nSystem Reliability Sampling-based 100% coverage Complete monitoring\nThe architecture works like this:\nFirst Tier: A fast classifier with 96% recall flags suspicious content\nSecond Tier: A reasoning model with 84% recall analyzes flagged interactions for actual threats\nBut the technical defenses tell only part of the story. OpenAI made difficult security choices that acknowledge some AI operations require significant restrictions for safe autonomous execution.\nBased on the vulnerabilities discovered, OpenAI implemented the following countermeasures across their model:\nWatch Mode Activation: When ChatGPT Agent accesses sensitive contexts like banking or email accounts, the system freezes all activity if users navigate away. This is in direct response to data exfiltration attempts discovered during testing.\nMemory Features Disabled: Despite being a core functionality, memory is completely disabled at launch to prevent the incremental data leaking attacks red teamers demonstrated.\nTerminal Restrictions: Network access limited to GET requests only, blocking the command execution vulnerabilities researchers exploited.\nRapid Remediation Protocol: A new system that patches vulnerabilities within hours of discovery—developed after red teamers showed how quickly exploits could spread.\nDuring pre-launch testing alone, this system identified and resolved 16 critical vulnerabilities that red teamers had discovered.\nA biological risk wake-up call\nRed teamers revealed the potential that the ChatGPT Agent could be comprimnised and lead to greater biological risks. Sixteen experienced participants from the Red Teaming Network, each with biosafety-relevant PhDs, attempted to extract dangerous biological information. Their submissions revealed the model could synthesize published literature on modifying and creating biological threats.\nIn response to the red teamers’ findings, OpenAI classified ChatGPT Agent as “High capability” for biological and chemical risks, not because they found definitive evidence of weaponization potential, but as a precautionary measure based on red team findings. This triggered:\nAlways-on safety classifiers scanning 100% of traffic\nA topical classifier achieving 96% recall for biology-related content\nA reasoning monitor with 84% recall for weaponization content\nA bio bug bounty program for ongoing vulnerability discovery\nWhat red teams taught OpenAI about AI security\nThe 110 attack submissions revealed patterns that forced fundamental changes in OpenAI’s security philosophy. They include the following:\nPersistence over power: Attackers don’t need sophisticated exploits, all they need is more time. Red teamers showed how patient, incremental attacks could eventually compromise systems.\nTrust boundaries are fiction: When your AI agent can access Google Drive, browse the web, and execute code, traditional security perimeters dissolve. Red teamers exploited the gaps between these capabilities.\nMonitoring isn’t optional: The discovery that sampling-based monitoring missed critical attacks led to the 100% coverage requirement.\nSpeed matters: Traditional patch cycles measured in weeks are worthless against prompt injection attacks that can spread instantly. The rapid remediation protocol patches vulnerabilities within hours.\nOpenAI is helping to create a new security baseline for Enterprise AI\nFor CISOs evaluating AI deployment, the red team discoveries establish clear requirements:\nQuantifiable protection: ChatGPT Agent’s 95% defense rate against documented attack vectors sets the industry benchmark. The nuances of the many tests and results defined in the system card explain the context of how they accomplished this and is a must-read for anyone involved with model security.\nComplete visibility: 100% traffic monitoring isn’t aspirational anymore. OpenAI’s experiences illustrate why it’s mandatory given how easily red teams can hide attacks anywhere.\nRapid response: Hours, not weeks, to patch discovered vulnerabilities.\nEnforced boundaries: Some operations (like memory access during sensitive tasks) must be disabled until proven safe.\nUK AISI’s testing proved particularly instructive. All seven universal attacks they identified were patched before launch, but their privileged access to internal systems revealed vulnerabilities that would eventually be discoverable by determined adversaries.\n“This is a pivotal moment for our Preparedness work,” Gu wrote on X. “Before we reached High capability, Preparedness was about analyzing capabilities and planning safeguards. Now, for Agent and future more capable models, Preparedness safeguards have become an operational requirement.”\nRed teams are core to building safer, more secure AI models\nThe seven universal exploits discovered by researchers and the 110 attacks from OpenAI’s red team network became the crucible that forged ChatGPT Agent.\nBy revealing exactly how AI agents could be weaponized, red teams forced the creation of the first AI system where security isn’t just a feature. It’s the foundation.\nChatGPT Agent’s results prove red teaming’s effectiveness: blocking 95% of visual browser attacks, catching 78% of data exfiltration attempts, monitoring every single interaction.\nIn the accelerating AI arms race, the companies that survive and thrive will be those who see their red teams as core architects of the platform that push it to the limits of safety and security.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/security/openais-red-team-plan-make-chatgpt-agent-an-ai-fortress/",
        "scraped_at": "2025-07-21T13:17:55.907090",
        "article_type": "regular"
    },
    {
        "title": "Meet AnyCoder, a new Kimi K2-powered tool for fast prototyping and deploying web apps",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nAnyCoder, an open-source web app development environment developed by Hugging Face ML Growth Lead Ahsen Khaliq (@_akhaliq on X), has launched on Hugging Face Spaces.\nThe tool, now available for all users of the AI code sharing repository Hugging Face, integrates live previews, multimodal input, and one-click deployment — all within a hosted environment, allowing indie creators without much technical expertise, or those working on behalf of clients or large enterprises, to get started “vibe coding” web apps rapidly using the assistance of Hugging Face-hosted AI models.\nFBI Veterans on AI Cyber Threats & Future Defenders\nIt also acts therefore as an alternative to services such as Lovable, which also allow users to type in plain English and begin coding apps without having formal programming knowledge.\nFree vibe coding available to all, powered by Kimi K2\nKhaliq built AnyCoder as a personal project within the Hugging Face ecosystem and as “one of the first vibe coding apps” to support Moonshot’s powerful yet small and efficient Kimi K2 model launched last week.\nAnyCoder’s main functionality allows users to enter plain-text descriptions to generate HTML, CSS, and JavaScript. These are displayed in a live preview pane and can be edited or directly deployed. It also includes example templates for todo apps, dashboards, calculators, and more.\nScreenshot of AnyCoder on Hugging Face\nBuilt entirely using Hugging Face’s open-source Python development environment Gradio, AnyCoder allows users to describe applications in plain English or upload images, and instantly generate working frontend code.\nKhaliq built AnyCoder as a personal project within the Hugging Face ecosystem.\nIn a direct message conversation with this VentureBeat journalist, he described it as a “free open source vibe coding app.”\nHowever, he also noted that multiple open source models are supported and users can switch between them with a dropdown menu on the control sidebar on the left pane, including:\nMoonshot Kimi-K2\nDeepSeek V3\nDeepSeek R1\nBaidu’s ERNIE-4.5-VL\nMiniMax M1\nAlibaba’s Qwen3-235B-A22B\nSmolLM3-3B\nGLM-4.1V-9B-Thinking\nCode from UI images, web search integration, and OCR support\nUsing the ERNIE-4.5-VL model, AnyCoder supports multimodal generation. Users can upload UI design screenshots or mockups and generate functional frontend code from them—making it useful for designers or teams working visually.\nADVERTISEMENT\nAnyCoder includes a website redesign tool that extracts content from any public site and re-renders it with a more modern layout. It uses scraped content like page structure, meta information, and images to build a new version, optionally guided by user instructions like “make it minimalist” or “add dark mode.”\nTo support up-to-date design trends and implementation patterns, AnyCoder offers web search integration via Tavily. When enabled with an API key, the platform searches for current technologies and best practices before generating code.\nUsers can upload images with embedded text—like screenshots or handwritten notes—and AnyCoder extracts that content using Tesseract OCR. The extracted text can then be incorporated into code prompts or app content.\nOne-click deployment to Hugging Face Spaces\nAnyCoder allows instant deployment of generated apps to Hugging Face Spaces. After authenticating via OAuth and granting the required permissions, users can deploy apps under their own Hugging Face account namespace. Deployments include:\nMobile-friendly, responsive designs\nBranded header/footer and README\nLive, shareable URL\nFull ownership and edit access\nADVERTISEMENT\nThis deployment capability now includes support for full Python apps built with Gradio, expanding the tool’s use cases beyond static sites. Support for Streamlit is also under development.\nFor novice developers or even those with technical expertise who want to spin up a new project fast, AnyCoder seems like a great and compelling place to start.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/programming-development/meet-anycoder-a-new-kimi-k2-powered-tool-for-fast-prototyping-and-deploying-web-apps/",
        "scraped_at": "2025-07-21T13:17:56.802842",
        "article_type": "regular"
    },
    {
        "title": "Salesforce used AI to cut support load by 5% — but the real win was teaching bots to say ‘I’m sorry’",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nSalesforce has crossed a significant threshold in the enterprise AI race, surpassing 1 million autonomous agent conversations on its help portal — a milestone that offers a rare glimpse into what it takes to deploy AI agents at massive scale and the surprising lessons learned along the way.\nThe achievement, confirmed by company executives in exclusive interviews with VentureBeat, comes just nine months after Salesforce launched Agentforce on its Help Portal in October. The platform now resolves 84% of customer queries autonomously, has led to a 5% reduction in support case volume, and enabled the company to redeploy 500 human support engineers to higher-value roles.\nFBI Veterans on AI Cyber Threats & Future Defenders\nBut perhaps more valuable than the raw numbers are the hard-won insights Salesforce gleaned from being what executives call “customer zero” for their own AI agent technology — lessons that challenge conventional wisdom about enterprise AI deployment and reveal the delicate balance required between technological capability and human empathy.\nHow Salesforce scaled from 126 to 45,000 AI conversations weekly using phased deployment\n“We started really small. We launched basically to a cohort of customers on our Help Portal. It had to be English to start with. You had to be logged in and we released it to about 10% of our traffic,” explains Bernard Slowey, SVP of Digital Customer Success at Salesforce, who led the Agentforce implementation. “The first week, I think there was 126 conversations, if I remember rightly. So me and my team could read through each one of them.”\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited: https://bit.ly/3GuuPLF\nThis methodical approach — starting with a controlled rollout before expanding to handle the current average of 45,000 conversations weekly — stands in stark contrast to the “move fast and break things” ethos often associated with AI deployment. The phased release allowed Salesforce to identify and fix critical issues before they could impact the broader customer base.\nThe technical foundation proved crucial. Unlike traditional chatbots that rely on decision trees and pre-programmed responses, Agentforce leverages Salesforce’s Data Cloud to access and synthesize information from 740,000 pieces of content across multiple languages and product lines.\n“The biggest difference here is, coming back to my data cloud thing is we were able to go out the gate and answer pretty much any question about any Salesforce product,” Slowey notes. “I don’t think we could have done it without data cloud.”\nWhy Salesforce taught its AI agents empathy after customers rejected cold, robotic responses\nOne of the most striking revelations from Salesforce’s journey involves what Joe Inzerillo, the company’s Chief Digital Officer, calls “the human part” of being a support agent.\n“When we first launched the agent, we were really concerned about, like, data factualism, you know, what is it getting the right data? Is it given the right answers and stuff like that? And what we realized is we kind of forgot about the human part,” Inzerillo reveals. “Somebody calls down and they’re like, hey, my stuff’s broken. I have a sub one incident right now, and you just come into like, ‘All right, well, I’ll open a ticket for you.’ It doesn’t feel great.”\nThis realization led to a fundamental shift in how Salesforce approached AI agent design. The company took its existing soft skills training program for human support engineers—what they call “the art of service” — and integrated it directly into Agentforce’s prompts and behaviors.\n“If you come now and say, ‘Hey, I’m having a Salesforce outage,’ Agentforce will apologize. ‘I’m so sorry. Like, that’s terrible. Let me get you through,’ and we’ll get that through to our engineering team,” Slowey explains. The impact on customer satisfaction was immediate and measurable.\nThe surprising reason Salesforce increased human handoffs from 1% to 5% for better customer outcomes\nPerhaps no metric better illustrates the complexity of deploying enterprise AI agents than Salesforce’s evolving approach to human handoffs. Initially, the company celebrated a 1% handoff rate — meaning only 1% of conversations were escalated from AI to human agents.\n“We were literally high fiving each other, going, ‘oh my god, like only 1%,'” Slowey recalls. “And then we look at the actual conversation. Was terrible. People were frustrated. They wanted to go to a human. The agent kept trying. It was just getting in the way.”\nThis led to a counterintuitive insight: making it harder for customers to reach humans actually degraded the overall experience. Salesforce adjusted its approach, and the handoff rate rose to approximately 5%.\n“I actually feel really good about that,” Slowey emphasizes. “If you want to create a case, you want to talk to a support engineer, that’s fine. Go ahead and do that.”\nInzerillo frames this as a fundamental shift in thinking about service metrics: “At 5% you really did get the vast, vast, vast majority in that 95% solved, and the people who didn’t got to a human faster. And so therefore their CSAT went up in the hybrid approach, where you had an agent and a human working together, you got better results than each of them had independently.”\nHow ‘content collisions’ forced Salesforce to delete thousands of help articles for AI accuracy\nSalesforce’s experience also revealed critical lessons about content management that many enterprises overlook when deploying AI. Despite having 740,000 pieces of content across multiple languages, the company discovered that abundance created its own problems.\n“There’s this words my team has been using that are new words to me, of content collisions,” Slowey explains. “Loads of password reset articles. And so it struggles on what’s the right article for me to take the chunks into Data Cloud and go to OpenAI and back and answer?”\nThis led to an extensive “content hygiene” initiative where Salesforce deleted outdated content, fixed inaccuracies, and consolidated redundant articles. The lesson: AI agents are only as good as the knowledge they can access, and sometimes less is more.\nThe Microsoft Teams integration that exposed why rigid AI guardrails backfire\nOne of the most enlightening mistakes Salesforce made involved being overly restrictive with AI guardrails. Initially, the company instructed Agentforce not to discuss competitors, listing every major rival by name.\n“We were worried people were going to come in and go, ‘is HubSpot better than Salesforce’ or something like that,” Slowey admits. But this created an unexpected problem: when customers asked legitimate questions about integrating Microsoft Teams with Salesforce, the agent refused to answer because Microsoft was on the competitor list.\nThe solution was elegantly simple: instead of rigid rules, Salesforce replaced the restrictive guardrails with a single instruction to “act in Salesforce’s best interest in everything you do.”\n“We realized we were still treating it like an old school chatbot, and what we needed to do is we needed to let the LLM be an LLM,” Slowey reflects.\nVoice interfaces and multilingual support drive Salesforce’s next phase of AI agent evolution\nLooking ahead, Salesforce is preparing for what both executives see as the next major evolution in AI agents: voice interfaces.\n“I actually believe voice is the UX of agents,” Slowey states. The company is developing iOS and Android native apps with voice capabilities, with plans to showcase them at Dreamforce later this year.\nInzerillo, drawing on his experience leading digital transformation at Disney, adds crucial context: “What’s important about voice is to understand that the chat is really foundational to the voice. Because chat, like, you still have to have all your information, you still have to have all those rules… If you jump right to voice, the real problem with voice is it’s got to be very fast and it’s got to be very accurate.”\nThe company has already expanded Agentforce to support Japanese using an innovative approach—rather than translating content, the system translates customer queries to English, retrieves relevant information, and translates responses back. With 87% resolution rates in Japanese after just three weeks, Salesforce plans to add French, German, Italian, and Spanish support by the end of July.\nFour critical lessons from Salesforce’s million-conversation journey for enterprise AI deployment\nFor enterprises considering their own AI agent deployments, Salesforce’s journey offers several critical insights:\nStart Small, Think Big: “Start small and then grow it out,” Slowey advises. The ability to review every conversation in early stages provides invaluable learning opportunities that would be impossible at scale.\nData Hygiene Matters: “Be really conscious of your data,” Inzerillo emphasizes. “Don’t over curate your data, but also don’t under curate your data and really think through, like, how do you best position the company?”\nEmbrace Flexibility: Traditional organizational structures may not align with AI capabilities. As Inzerillo notes, “If they try to take an agentic future and shove it into yesterday’s org chart, it’s going to be a very frustrating experience.”\nMeasure What Matters: Success metrics for AI agents differ from traditional support metrics. Response accuracy is important, but so are empathy, appropriate escalation, and overall customer satisfaction.\nThe billion-dollar question: what happens after you beat human performance?\nAs Salesforce’s AI agents now outperform human agents on key metrics like resolution rate and handle time, Inzerillo poses a thought-provoking question: “What do you measure after you beat the human?”\nThis question gets to the heart of what may be the most significant implication of Salesforce’s million-conversation milestone. The company isn’t just automating customer service—it’s redefining what good service looks like in an AI-first world.\n“We wanted to be the showcase to our customers and how we use Agentforce in our own experiences,” Slowey explains. “Part of why we do this… is so that we can learn these things, feed it back into our product teams, into our engineering teams to improve the product and then share these learnings with our customers.”\nWith enterprise spending on generative AI solutions projected to reach $143 billion by 2027, according to forecasts from International Data Corporation (IDC), Salesforce’s real-world lessons from the frontlines of deployment offer a crucial roadmap for organizations navigating their own AI transformations. Deloitte also estimates that global enterprise investments in generative AI could surpass $150 billion by 2027, reinforcing the scale and urgency of this technological shift.\nThe message is clear: success in the AI agent era requires more than just sophisticated technology. It demands a fundamental rethinking of how humans and machines work together, a commitment to continuous learning and iteration, and perhaps most surprisingly, a recognition that the most advanced AI agents are those that remember to be human.\nAs Slowey puts it: “You now have two employees. You have an agentic AI agent, and you have a human employee. You need to train both on the soft skills, the art of service.”\nIn the end, Salesforce’s million conversations may be less about the milestone itself and more about what it represents: the emergence of a new paradigm where digital labor doesn’t replace human work but transforms it, creating possibilities that neither humans nor machines could achieve alone.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/ai/salesforce-used-ai-to-cut-support-load-by-5-but-the-real-win-was-teaching-bots-to-say-im-sorry/",
        "scraped_at": "2025-07-21T13:17:57.607288",
        "article_type": "regular"
    },
    {
        "title": "Mistral’s Le Chat adds deep research agent and voice mode to challenge OpenAI’s enterprise dominance",
        "content": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nSince OpenAI introduced Deep Research, an AI agent that can conduct research for users and generate a comprehensive report, many other companies have released their own versions of this capability, all named Deep Research. \nDeep Research, as a feature and product, can be accessed through various platforms, including Google’s Gemini, AlphaSense, You.com, DeepSeek, Grok 3 and many others.  Now, French company Mistral joins the fray with the launch of deep research capabilities into its Le Chat, among other updates to the platform. \nFBI Veterans on AI Cyber Threats & Future Defenders\nIn a blog post, the company said Deep Research and other new features will make Le Chat “even more capable, more intuitive and more fun.”\nLe Chat users can open research mode and ask it something. The chatbot then asks questions to clarify some information and then begins gathering sources. It will put together “a structured, reference-backed report that’s easy to follow.”\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited: https://bit.ly/3GuuPLF\nMistral said its research is powered by a Deep Research agent, which it designed to be “genuinely helpful” and feels like working with an organized research partner. \nDeep Research has been called “the first mass market AI that could displace jobs,” especially since it can put out reports faster than human analysts. \nMistral also updated “thinking mode,” where Le Chat users can access the company’s chain-of-thought model Magistral, to read and respond in different languages. It can also code-switch mid-sentence. \nPrompt-based image editing and other features\nFor people creating images on the chat app, they can ask the chatbot to edit parts of the photo with just a prompt. Users can say something like “generate a drawing of a cat,” then ask Le Chat to “place him in Istanbul,” and it will do just that. \n“It’s ideal for making consistent edits across a series, keeping people, objects, and design elements recognizable from one image to the next,” Mistral said. \nThanks to the recently released speech recognition model, Voxtral, Le Chat can now support voice mode, where users can chat out loud with the platform. The company said this mode is best for low-latency speech recognition and keeping up with someone’s conversational pace. \nLe Chat’s new Projects feature allows users to organize related conversations and topics into groups. The projects will utilize their own libraries — which can include uploaded files — and retain tools and preferred settings. This is similar to Google’s NotebookLM. \nPlaying catch-up\nMany of the new features on Le Chat may seem familiar. It’s normal for other chat platforms to introduce similar features, especially as people begin to expect these capabilities when using chat systems. \nFor example, both Gemini and ChatGPT allow users to edit generated photos using a prompt. Sometimes, the chatbots misunderstand and redo the entire image. However, I recently generated and edited a photo with ChatGPT, and the chatbot removed exactly what I wanted it to. \nVoice mode has been available on ChatGPT since September 2024, though ChatGPT always included a “Read Aloud” feature. Project Astra from Google took voice mode to a new level, demonstrating in a demo that users can point out something in the physical world to Gemini and ask it to describe it out loud. \nHowever, Mistral has the advantage of being Europe-based and can bring features directly to the European market. Companies like OpenAI often struggle to bring certain services, such as ChatGPT’s Advanced Voice Mode, to Europe due to data regulations and some provisions of the European Union’s AI Act. \nDespite this, users seemed excited that Mistral brought many new powerful features to Le Chat, with some early users seeing strong performance from Mistral’s Deep Research. \nHere is the first page of the 8 page state of the art written by Le Chat of @MistralAI in its new deep research mode. Just 5 minutes with a great interface for this task. I think that is a pretty good job. pic.twitter.com/jeUAy1U6Yo\n— Eduardo C. Garrido-Merchan (@vedugarmer) July 17, 2025\nTime to rename Le Chat to L’Assistant\n— Mev-Rael (@Mevrael) July 17, 2025\n@realJohnSpyker I am no longer using grok. i am switching to Le Chat by Mistral AI AI assistant for life and work. Find answers, generate images and read the news.\n\nLe Chat combines the power of advanced AI with extensive information sourced from the web\n— Tsunami Papi #WineDad (@SuriTsunami) July 17, 2025\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our Privacy Policy",
        "source_url": "https://venturebeat.com/ai/mistrals-le-chat-adds-deep-research-agent-and-voice-mode-to-challenge-openais-enterprise-dominance/",
        "scraped_at": "2025-07-21T13:17:59.716482",
        "article_type": "regular"
    }
]